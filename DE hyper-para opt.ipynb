{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import time\n",
    "plt.style.use('seaborn-notebook')\n",
    "%matplotlib inline\n",
    "\n",
    "# JUST TO MAKE SURE SOME WARNINGS ARE IGNORED \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from logging import getLogger\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "class DECore(object):\n",
    "    \"\"\"\n",
    "    Core Class of Differential Evolution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 objective_function: callable,\n",
    "                 ndim: int,\n",
    "                 lower_limit: np.ndarray,\n",
    "                 upper_limit: np.ndarray,\n",
    "                 minimize: bool = True):\n",
    "\n",
    "        \"\"\"\n",
    "        :param objective_function: f(x) callable function\n",
    "        :param ndim: dimension of x\n",
    "        :param lower_limit: lower limit of search space 1d-array\n",
    "        :param upper_limit: upper limit of search space 1d-array\n",
    "        :param minimize: minimize flag. if the problem is minimization, then set True.\n",
    "                                        otherwise set False and turning as maximization.\n",
    "        \"\"\"\n",
    "        self._of = objective_function\n",
    "        self._pop = None\n",
    "        self._nd = ndim\n",
    "        self._x_current = None\n",
    "        self._low_lim = lower_limit\n",
    "        self._up_lim = upper_limit\n",
    "        self._f_current = None\n",
    "        self._is_minimize = minimize\n",
    "        self._orbit = None\n",
    "\n",
    "    def initialization(self, x_init=None):\n",
    "        \"\"\"\n",
    "        :param x_init: initial value of x (optional)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # initialize x\n",
    "        if x_init:\n",
    "            self._x_current = x_init\n",
    "        else:\n",
    "            self._x_current = np.random.rand(self._pop, self._nd) * (self._up_lim - self._low_lim) + self._low_lim\n",
    "\n",
    "        # initialize orbit\n",
    "        self._orbit = []\n",
    "\n",
    "    def _selection(self, **kwargs):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _mutation(self, **kwargs):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _crossover(self, **kwargs):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _mutation_crossover(self, **kwargs):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _evaluate_with_check(self, x):\n",
    "        if np.any(x < self._low_lim) or np.any(x > self._up_lim):\n",
    "            return np.inf if self._is_minimize else -np.inf\n",
    "        else:\n",
    "            try:\n",
    "                f = self._of(x)\n",
    "            except Exception as ex:\n",
    "                logger.error(ex)\n",
    "                f = np.inf if self._is_minimize else -np.inf\n",
    "            return f\n",
    "\n",
    "    def _evaluate(self, params):\n",
    "        current, u = params\n",
    "        return current, self._evaluate_with_check(u)\n",
    "\n",
    "    def optimize_mp(self, **kwargs):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def optimize(self, **kwargs):\n",
    "\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def orbit(self):\n",
    "        return self._orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from concurrent import futures\n",
    "from logging import getLogger\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "class DE(DECore):\n",
    "    \"\"\"\n",
    "    Differential Evolution\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 objective_function: callable,\n",
    "                 ndim: int,\n",
    "                 lower_limit: np.ndarray,\n",
    "                 upper_limit: np.ndarray,\n",
    "                 minimize: bool = True):\n",
    "        \"\"\"\n",
    "        :param objective_function: f(x) callable function\n",
    "        :param ndim: dimension of x\n",
    "        :param lower_limit: lower limit of search space 1d-array\n",
    "        :param upper_limit: upper limit of search space 1d-array\n",
    "        :param minimize: minimize flag. if the problem is minimization, then set True.\n",
    "                                        otherwise set False and turning as maximization.\n",
    "        \"\"\"\n",
    "\n",
    "        super(DE, self).__init__(objective_function=objective_function,\n",
    "                                 ndim=ndim,\n",
    "                                 lower_limit=lower_limit,\n",
    "                                 upper_limit=upper_limit,\n",
    "                                 minimize=minimize)\n",
    "\n",
    "    def _selection(self, p, u):\n",
    "        \"\"\"\n",
    "        :param p: current index\n",
    "        :param u: trial vectors\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        fu = self._evaluate_with_check(u)\n",
    "\n",
    "        # score is better than current\n",
    "        q1 = fu <= self._f_current[p] if self._is_minimize else fu >= self._f_current[p]\n",
    "        # over lower limit\n",
    "        q2 = np.any(u < self._low_lim)\n",
    "        # over upper limit\n",
    "        q3 = np.any(u > self._up_lim)\n",
    "        # q1 ^ ~q2 ^ ~q3\n",
    "        q = q1 * ~q2 * ~q3\n",
    "\n",
    "        f_p1 = fu if q else self._f_current[p]\n",
    "        x_p1 = u if q else self._x_current[p]\n",
    "        return p, f_p1, x_p1\n",
    "\n",
    "    def _mutation(self, current, mutant, num, sf):\n",
    "        \"\"\"\n",
    "        :param current: current index of population\n",
    "        :param mutant: mutation method\n",
    "        :param num: number of mutant vectors\n",
    "        :param sf: scaling factor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        assert num > 0, \"'num' must be greater than 0.\"\n",
    "\n",
    "        # mutant vector\n",
    "        # best\n",
    "        if mutant == 'best':\n",
    "            r_best = np.argmin(self._f_current) if self._is_minimize else np.argmax(self._f_current)\n",
    "            r = [r_best]\n",
    "            r += np.random.choice([n for n in range(self._pop) if n != r_best], 2 * num, replace=False).tolist()\n",
    "            v = self._x_current[r[0]] \\\n",
    "                + sf * np.sum([self._x_current[r[m + 1]] - self._x_current[r[m + 2]] for m in range(num)], axis=0)\n",
    "\n",
    "        # rand\n",
    "        elif mutant == 'rand':\n",
    "            r = np.random.choice(range(self._pop), 2 * num + 1, replace=False).tolist()\n",
    "            v = self._x_current[r[0]] \\\n",
    "                + sf * np.sum([self._x_current[r[m + 1]] - self._x_current[r[m + 2]] for m in range(num)], axis=0)\n",
    "\n",
    "        # current-to-rand\n",
    "        elif mutant == 'current-to-rand':\n",
    "            r = [current]\n",
    "            r += np.random.choice([n for n in range(self._pop) if n != current], 2 * num + 1, replace=False).tolist()\n",
    "            v = self._x_current[r[0]] \\\n",
    "                + sf * (self._x_current[r[1]] - self._x_current[r[0]]) \\\n",
    "                + sf * np.sum([self._x_current[r[m + 2]] - self._x_current[r[m + 3]] for m in range(num)], axis=0)\n",
    "\n",
    "        # current-to-best\n",
    "        elif mutant == 'current-to-best':\n",
    "            r_best = np.argmin(self._f_current) if self._is_minimize else np.argmax(self._f_current)\n",
    "            r = [r_best, current]\n",
    "            r += np.random.choice([\n",
    "                n for n in range(self._pop) if n not in [r_best, current]], 2 * num, replace=False).tolist()\n",
    "            v = self._x_current[r[0]] \\\n",
    "                + sf * (self._x_current[r[1]] - self._x_current[r[0]]) \\\n",
    "                + sf * np.sum([self._x_current[r[m + 2]] - self._x_current[r[m + 3]] for m in range(num)], axis=0)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('invalid `mutant`: {}'.format(mutant))\n",
    "\n",
    "        return v\n",
    "\n",
    "    def _crossover(self, v, x, cross, cr):\n",
    "        \"\"\"\n",
    "        :param v: mutant vector\n",
    "        :param x: current vector\n",
    "        :param cross: crossover method\n",
    "        :param cr: crossover-rate\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # crossover\n",
    "        r = np.random.choice(range(self._nd))\n",
    "        u = np.zeros(self._nd)\n",
    "\n",
    "        # binary crossover\n",
    "        if cross == 'bin':\n",
    "            flg = np.equal(r, np.arange(self._nd)) + np.random.rand(self._nd) < cr\n",
    "\n",
    "        # exponential crossover\n",
    "        elif cross == 'exp':\n",
    "            flg = np.array([False for _ in range(self._nd)])\n",
    "            for l in range(self._nd):\n",
    "                flg[r] = True\n",
    "                r = (r + 1) % self._nd\n",
    "                if np.random.rand() >= cr:\n",
    "                    break\n",
    "        else:\n",
    "            raise ValueError('invalid `cross`: {}'.format(cross))\n",
    "\n",
    "        # from mutant vector\n",
    "        u[flg] = v[flg]\n",
    "        # from current vector\n",
    "        u[~flg] = x[~flg]\n",
    "\n",
    "        return u\n",
    "\n",
    "    def _mutation_crossover(self, mutant, num, sf, cross, cr):\n",
    "        l_up = []\n",
    "        # for each individuals\n",
    "        for p in range(self._pop):\n",
    "            # mutation\n",
    "            v_p = self._mutation(p, mutant=mutant, num=num, sf=sf)\n",
    "\n",
    "            # crossover\n",
    "            u_p = self._crossover(v_p, self._x_current[p], cross=cross, cr=cr)\n",
    "            l_up.append(u_p)\n",
    "\n",
    "        return l_up\n",
    "\n",
    "    def optimize_mp(self,\n",
    "                    k_max: int,\n",
    "                    population: int = 5,\n",
    "                    mutant: str = 'best',\n",
    "                    num: int = 1,\n",
    "                    cross: str = 'bin',\n",
    "                    sf: float = 0.7,\n",
    "                    cr: float = 0.3,\n",
    "                    proc: [int, None] = None):\n",
    "        \"\"\"\n",
    "        :param k_max: max-iterations\n",
    "        :param population: number of populations\n",
    "        :param mutant: mutation method ['best', 'rand', 'current-to-best', 'current-to-rand']\n",
    "        :param num: number of mutant vectors\n",
    "        :param cross: crossover method ['bin', 'exp']\n",
    "        :param sf: scaling-factor F\n",
    "        :param cr: crossover-rate CR\n",
    "        :param proc: number of process. if None, then use maximum process\n",
    "        :return:\n",
    "        ex) DE/rand/1/bin --> method='rand', num=1, cross='bin'\n",
    "            DE/best/2/exp --> method='best', num=2, cross='exp'\n",
    "        \"\"\"\n",
    "        # set population\n",
    "        self._pop = population\n",
    "\n",
    "        # initialize\n",
    "        self.initialization()\n",
    "\n",
    "        # get fitness of initial x\n",
    "        with futures.ProcessPoolExecutor(proc) as executor:\n",
    "            results = executor.map(self._evaluate, zip(range(self._pop), self._x_current))\n",
    "   \n",
    "        self._f_current = np.array([r[1] for r in sorted(list(results))])\n",
    "\n",
    "        for k in range(k_max):\n",
    "            # mutation and crossover\n",
    "            l_up = self._mutation_crossover(mutant, num, sf, cross, cr)\n",
    "\n",
    "            # multi-processing\n",
    "            with futures.ProcessPoolExecutor(proc) as executor:\n",
    "                results = executor.map(self._selection, range(self._pop), l_up)\n",
    "\n",
    "            # correct results\n",
    "            _x_current = []\n",
    "            _f_current = []\n",
    "            for _, fp, x in sorted(results):\n",
    "                _x_current.append(x)\n",
    "                _f_current.append(fp)\n",
    "\n",
    "            # update current values\n",
    "            self._x_current = np.r_[_x_current].copy()\n",
    "            self._f_current = np.array(_f_current).copy()\n",
    "\n",
    "            best_score = np.amin(self._f_current) if self._is_minimize else np.amax(self._f_current)\n",
    "            logger.info('k={} best score = {}'.format(k, best_score))\n",
    "            self._orbit.append(best_score)\n",
    "\n",
    "        # get best point\n",
    "        best_idx = np.argmin(self._f_current) if self._is_minimize else np.argmax(self._f_current)\n",
    "        x_best = self._x_current[best_idx]\n",
    "        logger.info('global best score = {}'.format(self._f_current[best_idx]))\n",
    "        logger.info('x_best = {}'.format(x_best))\n",
    "        \n",
    "        return x_best\n",
    "\n",
    "    def optimize(self,\n",
    "                 k_max: int,\n",
    "                 population: int = 10,\n",
    "                 mutant: str = 'best',\n",
    "                 num: int = 1,\n",
    "                 cross: str = 'bin',\n",
    "                 sf: float = 0.7,\n",
    "                 cr: float = 0.3):\n",
    "        \"\"\"\n",
    "        :param k_max: max-iterations\n",
    "        :param population: number of populations\n",
    "        :param mutant: mutation method ['best', 'rand', 'current-to-best', 'current-to-rand']\n",
    "        :param num: number of mutant vectors\n",
    "        :param cross: crossover method ['bin', 'exp']\n",
    "        :param sf: scaling-factor F\n",
    "        :param cr: crossover-rate CR\n",
    "        :return:\n",
    "        ex) DE/rand/1/bin --> method='rand', num=1, cross='bin'\n",
    "            DE/best/2/exp --> method='best', num=2, cross='exp'\n",
    "        \"\"\"\n",
    "        # set population\n",
    "        self._pop = population\n",
    "\n",
    "        # initialize\n",
    "        self.initialization()\n",
    "\n",
    "        # get fitness of initial x\n",
    "        self._f_current = np.array([self._evaluate_with_check(x) for x in self._x_current])\n",
    "\n",
    "        for k in range(k_max):\n",
    "            # mutation and crossover\n",
    "            l_up = self._mutation_crossover(mutant, num, sf, cross, cr)\n",
    "\n",
    "            for p, u_p in enumerate(l_up):\n",
    "                # selection\n",
    "                _, f_p1, x_p1 = self._selection(p, u_p)\n",
    "\n",
    "                # update current values\n",
    "                self._f_current[p] = f_p1\n",
    "                self._x_current[p] = x_p1\n",
    "\n",
    "            best_score = np.amin(self._f_current) if self._is_minimize else np.amax(self._f_current)\n",
    "            logger.info('k={} best score = {}'.format(k, best_score))\n",
    "            self._orbit.append(best_score)\n",
    "\n",
    "        # get best point\n",
    "        best_idx = np.argmin(self._f_current) if self._is_minimize else np.argmax(self._f_current)\n",
    "        x_best = self._x_current[best_idx]\n",
    "        logger.info('global best score = {}'.format(self._f_current[best_idx]))\n",
    "        logger.info('x_best = {}'.format(x_best))\n",
    "        return x_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ag_data = pd.read_csv('Ag_ordered_dataset.csv',  index_col = 0)\n",
    "shape_set = Ag_data['Shape'].unique()\n",
    "\n",
    "# Use label encoder to transform the label shape\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(shape_set)\n",
    "normalised_shape = le.transform(Ag_data['Shape']) \n",
    "\n",
    "Ag_data['Shape'] = normalised_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Ag_data.iloc[:, :79]\n",
    "y = Ag_data['Shape']\n",
    "y = np.array(y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 ... 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "dataset = load_digits()\n",
    "print(dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:k=0 best score = 0.7566643882433357\n",
      "INFO:__main__:k=1 best score = 0.7566643882433357\n",
      "INFO:__main__:k=2 best score = 0.7566643882433357\n",
      "INFO:__main__:k=3 best score = 0.7566643882433357\n",
      "INFO:__main__:k=4 best score = 0.7566643882433357\n",
      "INFO:__main__:k=5 best score = 0.7566643882433357\n",
      "INFO:__main__:k=6 best score = 0.7591934381408066\n",
      "INFO:__main__:k=7 best score = 0.7591934381408066\n",
      "INFO:__main__:k=8 best score = 0.7799384825700616\n",
      "INFO:__main__:k=9 best score = 0.7799384825700616\n",
      "INFO:__main__:k=10 best score = 0.7799384825700616\n",
      "INFO:__main__:k=11 best score = 0.7799384825700616\n",
      "INFO:__main__:k=12 best score = 0.7799384825700616\n",
      "INFO:__main__:k=13 best score = 0.7799384825700616\n",
      "INFO:__main__:k=14 best score = 0.7799384825700616\n",
      "INFO:__main__:k=15 best score = 0.7799384825700616\n",
      "INFO:__main__:k=16 best score = 0.7799384825700616\n",
      "INFO:__main__:k=17 best score = 0.7799384825700616\n",
      "INFO:__main__:k=18 best score = 0.7799384825700616\n",
      "INFO:__main__:k=19 best score = 0.7799384825700616\n",
      "INFO:__main__:k=20 best score = 0.7799384825700616\n",
      "INFO:__main__:k=21 best score = 0.7799384825700616\n",
      "INFO:__main__:k=22 best score = 0.7799384825700616\n",
      "INFO:__main__:k=23 best score = 0.7800410116199591\n",
      "INFO:__main__:k=24 best score = 0.7800410116199591\n",
      "INFO:__main__:k=25 best score = 0.7800410116199591\n",
      "INFO:__main__:k=26 best score = 0.7800410116199591\n",
      "INFO:__main__:k=27 best score = 0.7800410116199591\n",
      "INFO:__main__:k=28 best score = 0.7800410116199591\n",
      "INFO:__main__:k=29 best score = 0.7800410116199591\n",
      "INFO:__main__:k=30 best score = 0.7800410116199591\n",
      "INFO:__main__:k=31 best score = 0.7800410116199591\n",
      "INFO:__main__:k=32 best score = 0.7800410116199591\n",
      "INFO:__main__:k=33 best score = 0.7800410116199591\n",
      "INFO:__main__:k=34 best score = 0.7800410116199591\n",
      "INFO:__main__:k=35 best score = 0.7800410116199591\n",
      "INFO:__main__:k=36 best score = 0.7800410116199591\n",
      "INFO:__main__:k=37 best score = 0.7800410116199591\n",
      "INFO:__main__:k=38 best score = 0.7800410116199591\n",
      "INFO:__main__:k=39 best score = 0.7800410116199591\n",
      "INFO:__main__:k=40 best score = 0.7800410116199591\n",
      "INFO:__main__:k=41 best score = 0.7800410116199591\n",
      "INFO:__main__:k=42 best score = 0.7800410116199591\n",
      "INFO:__main__:k=43 best score = 0.7800410116199591\n",
      "INFO:__main__:k=44 best score = 0.7800410116199591\n",
      "INFO:__main__:k=45 best score = 0.7800410116199591\n",
      "INFO:__main__:k=46 best score = 0.7800410116199591\n",
      "INFO:__main__:k=47 best score = 0.7800410116199591\n",
      "INFO:__main__:k=48 best score = 0.7800410116199591\n",
      "INFO:__main__:k=49 best score = 0.7800410116199591\n",
      "INFO:__main__:k=50 best score = 0.7800410116199591\n",
      "INFO:__main__:k=51 best score = 0.7800410116199591\n",
      "INFO:__main__:k=52 best score = 0.7800410116199591\n",
      "INFO:__main__:k=53 best score = 0.7800410116199591\n",
      "INFO:__main__:k=54 best score = 0.7800410116199591\n",
      "INFO:__main__:k=55 best score = 0.7800410116199591\n",
      "INFO:__main__:k=56 best score = 0.7800410116199591\n",
      "INFO:__main__:k=57 best score = 0.7800410116199591\n",
      "INFO:__main__:k=58 best score = 0.7800410116199591\n",
      "INFO:__main__:k=59 best score = 0.7800410116199591\n",
      "INFO:__main__:k=60 best score = 0.7800410116199591\n",
      "INFO:__main__:k=61 best score = 0.7800410116199591\n",
      "INFO:__main__:k=62 best score = 0.7800410116199591\n",
      "INFO:__main__:k=63 best score = 0.7800410116199591\n",
      "INFO:__main__:k=64 best score = 0.7800410116199591\n",
      "INFO:__main__:k=65 best score = 0.7800410116199591\n",
      "INFO:__main__:k=66 best score = 0.7800410116199591\n",
      "INFO:__main__:k=67 best score = 0.7800410116199591\n",
      "INFO:__main__:k=68 best score = 0.7800410116199591\n",
      "INFO:__main__:k=69 best score = 0.7800410116199591\n",
      "INFO:__main__:k=70 best score = 0.7800410116199591\n",
      "INFO:__main__:k=71 best score = 0.7800410116199591\n",
      "INFO:__main__:k=72 best score = 0.7800410116199591\n",
      "INFO:__main__:k=73 best score = 0.7800410116199591\n",
      "INFO:__main__:k=74 best score = 0.7800410116199591\n",
      "INFO:__main__:k=75 best score = 0.7800410116199591\n",
      "INFO:__main__:k=76 best score = 0.7800410116199591\n",
      "INFO:__main__:k=77 best score = 0.7800410116199591\n",
      "INFO:__main__:k=78 best score = 0.7800410116199591\n",
      "INFO:__main__:k=79 best score = 0.7800410116199591\n",
      "INFO:__main__:k=80 best score = 0.7800410116199591\n",
      "INFO:__main__:k=81 best score = 0.7800410116199591\n",
      "INFO:__main__:k=82 best score = 0.7881408065618591\n",
      "INFO:__main__:k=83 best score = 0.7881408065618591\n",
      "INFO:__main__:k=84 best score = 0.7881408065618591\n",
      "INFO:__main__:k=85 best score = 0.7881408065618591\n",
      "INFO:__main__:k=86 best score = 0.7881408065618591\n",
      "INFO:__main__:k=87 best score = 0.7881408065618591\n",
      "INFO:__main__:k=88 best score = 0.7881408065618591\n",
      "INFO:__main__:k=89 best score = 0.7881408065618591\n",
      "INFO:__main__:k=90 best score = 0.7881408065618591\n",
      "INFO:__main__:k=91 best score = 0.7881408065618591\n",
      "INFO:__main__:k=92 best score = 0.7881408065618591\n",
      "INFO:__main__:k=93 best score = 0.7881408065618591\n",
      "INFO:__main__:k=94 best score = 0.7881408065618591\n",
      "INFO:__main__:k=95 best score = 0.7881408065618591\n",
      "INFO:__main__:k=96 best score = 0.7881408065618591\n",
      "INFO:__main__:k=97 best score = 0.7881408065618591\n",
      "INFO:__main__:k=98 best score = 0.7881408065618591\n",
      "INFO:__main__:k=99 best score = 0.7881408065618591\n",
      "INFO:__main__:global best score = 0.7881408065618591\n",
      "INFO:__main__:x_best = [9.24452012 7.76876382 2.671719   3.17274613]\n",
      "INFO:__main__:best parameter = {'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "time cost 44.65617322921753 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tempfile import TemporaryDirectory\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from logging import getLogger, basicConfig\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "class HyperTuner(object):\n",
    "    def __init__(self, model, space: dict, k_fold: int = 5, **params):\n",
    "        \"\"\"\n",
    "        :param model: target model\n",
    "        :param space: search space\n",
    "        :param k_fold: number of folders for K-fold CV\n",
    "        :param params: parameters for optimizer\n",
    "        space = {\n",
    "            'parameter': {'scale': linear', 'range': [0, 1.5]},\n",
    "            'parameter': {'scale': 'log', 'range': [-1, 2]},\n",
    "            'parameter': {'scale': 'category', 'range': ['a', 'b', 'c']},\n",
    "            'parameter': {'scale': 'integer', 'range': [0, 10]},\n",
    "            'parameter': 'static'\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "        assert isinstance(space, dict)\n",
    "        self._space = space\n",
    "        self._parameters = list(self._space.keys())\n",
    "        self._static_params = [p for p in self._parameters if not isinstance(self._space[p], dict)]\n",
    "        self._variable_params = [p for p in self._parameters if isinstance(self._space[p], dict)]\n",
    "        self._tempdir = TemporaryDirectory()\n",
    "        self._tempfile = Path(self._tempdir.name + 'temp_data.gz')\n",
    "        self._eval_function = None\n",
    "        default_opt_param = {'k_max': 100,\n",
    "                             'population': 10,\n",
    "                             'mutant': 'best',\n",
    "                             'num': 1,\n",
    "                             'cross': 'bin',\n",
    "                             'sf': 0.7,\n",
    "                             'cr': 0.4}\n",
    "        self._optimizer_param = default_opt_param\n",
    "        self._optimizer_param.update(params)\n",
    "        self._kf = k_fold\n",
    "\n",
    "    def __del__(self):\n",
    "        self._tempdir.cleanup()\n",
    "\n",
    "    def _get_search_limits(self):\n",
    "        lowers = []\n",
    "        uppers = []\n",
    "        for k in self._variable_params:\n",
    "            if self._space[k]['scale'] in ['linear', 'log']:\n",
    "                lowers.append(self._space[k]['range'][0])\n",
    "                uppers.append(self._space[k]['range'][1])\n",
    "            elif self._space[k]['scale'] == 'integer':\n",
    "                lowers.append(self._space[k]['range'][0])\n",
    "                uppers.append(self._space[k]['range'][1] + 1)\n",
    "            else:\n",
    "                lowers.append(0)\n",
    "                uppers.append(len(self._space[k]['range']))\n",
    "\n",
    "        return np.array(lowers), np.array(uppers)\n",
    "\n",
    "    def _translate_to_origin(self, x):\n",
    "        org_x = {}\n",
    "        for n, k in enumerate(self._variable_params):\n",
    "            if self._space[k]['scale'] == 'log':\n",
    "                org_x[k] = np.power(10, x[n])\n",
    "            elif self._space[k]['scale'] == 'category':\n",
    "                org_x[k] = self._space[k]['range'][int(x[n])]\n",
    "            elif self._space[k]['scale'] == 'integer':\n",
    "                org_x[k] = int(x[n])\n",
    "            else:\n",
    "                org_x[k] = x[n]\n",
    "\n",
    "        # static parameters\n",
    "        for k in self._static_params:\n",
    "            org_x[k] = self._space[k]\n",
    "        return org_x\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        # load data from temporary directory\n",
    "        input_data, targets = joblib.load(self._tempfile)\n",
    "\n",
    "        # set model using parameter x\n",
    "        param = self._translate_to_origin(x)\n",
    "        model = self._model.set_params(**param)\n",
    "\n",
    "        # train model using CV (K-fold)\n",
    "        skf = KFold(n_splits=self._kf, shuffle=True)\n",
    "        scores = []\n",
    "        for train, test in skf.split(input_data, targets):\n",
    "            x_tr, t_tr = input_data[train], targets[train]\n",
    "            x_te, t_te = input_data[test], targets[test]\n",
    "\n",
    "            model.fit(x_tr, t_tr)\n",
    "            scores.append(self._eval_function(y_pred=model.predict(x_te), y_true=t_te))\n",
    "\n",
    "        # average score\n",
    "        return np.average(scores)\n",
    "\n",
    "    def tuning(self, eval_function: callable, x: np.ndarray, t: np.ndarray, minimize: bool = True):\n",
    "        joblib.dump((x, t), self._tempfile)\n",
    "\n",
    "        # set DE\n",
    "        lower_limit, upper_limit = self._get_search_limits()\n",
    "\n",
    "        # set evaluation function\n",
    "        self._eval_function = eval_function\n",
    "        optimizer = DE(objective_function=self._evaluate, ndim=len(lower_limit), lower_limit=lower_limit,\n",
    "                       upper_limit=upper_limit, minimize=minimize)\n",
    "\n",
    "        x_best = optimizer.optimize_mp(**self._optimizer_param)\n",
    "\n",
    "        return self._translate_to_origin(x_best)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    basicConfig(level='INFO')\n",
    "\n",
    "    from sklearn.datasets import load_digits\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    search_space = {\n",
    "        'max_depth': {'scale': 'integer', 'range': [1, 10]},\n",
    "        'min_samples_split': {'scale': 'integer', 'range': [2, 10]},\n",
    "        'min_samples_leaf': {'scale': 'integer', 'range': [1, 10]},\n",
    "        'max_features': {'scale': 'category', 'range': ['auto', 'sqrt', 'log2', None]}\n",
    "    }\n",
    "\n",
    "    tuner = HyperTuner(model= RandomForestClassifier(random_state =1), space=search_space)\n",
    "    time_start=time.time()\n",
    "    best_param = tuner.tuning(eval_function=accuracy_score, x=X_train, t=y_train, minimize=False)\n",
    "    time_end=time.time()\n",
    "    logger.info('best parameter = {}'.format(best_param))\n",
    "    \n",
    "    print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 10.26343035697937 s\n"
     ]
    }
   ],
   "source": [
    "search_space = {'max_depth': [1,2,3,4,5,6,7,8,9,10],\n",
    "                'min_samples_split':[2,3,4,5,6,7,8,9,10],\n",
    "                'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10],\n",
    "                'max_features': ('auto', 'sqrt', 'log2', None)}\n",
    "\n",
    "time_start=time.time()\n",
    "clf = RandomizedSearchCV(RandomForestClassifier(random_state = 1), search_space, cv = 5, n_iter = 100)\n",
    "clf.fit(X, y)\n",
    "time_end=time.time()\n",
    "\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': None, 'max_depth': 9}\n",
      "0.7482352941176471\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost 366.5881428718567 s\n"
     ]
    }
   ],
   "source": [
    "time_start=time.time()\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state = 1), search_space, cv = 5)\n",
    "clf.fit(X_train, y_train)\n",
    "time_end=time.time()\n",
    "\n",
    "print('time cost',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.7696335078534031\n"
     ]
    }
   ],
   "source": [
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth = 10, max_features = None, min_samples_leaf = 1, min_samples_split = 3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=3,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7209302325581395"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth = 9, max_features = None, min_samples_leaf = 2, min_samples_split = 7, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7209302325581395"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
